{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586914f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 算法 keras 自定义loss  Matlab data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import tensorflow_probability as tfp\n",
    "import scipy.io as scio\n",
    "\n",
    "# DeepNN topology\n",
    "layers=[2,20,20,20,20,20,20,3]\n",
    "# layers=[2,50,50,20,1]\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "   learning_rate=0.001, beta_1=0.99, epsilon=1e-1)\n",
    "# tf_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# sampling points\n",
    "dataFile = \"x1\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x1 = data['x1']\n",
    "dataFile = \"x2\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "x2 = data['x2']\n",
    "x1 =tf.reduce_sum(x1, axis=0)\n",
    "x2 =tf.reduce_sum(x2, axis=0)\n",
    "x1=tf.cast(x1, dtype=tf.float32)\n",
    "x2=tf.cast(x2, dtype=tf.float32)\n",
    "#print(x1)\n",
    "\n",
    "class mymodel():\n",
    "    def __init__(self,layers,optimizer,x1,x2):\n",
    "        self.u_model=tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(lambda X: X))\n",
    "        for width in layers[1:7]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "            width, activation=tf.nn.tanh,\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        self.u_model.add(tf.keras.layers.Dense(\n",
    "            layers[7],\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "        \n",
    "        self.optimizer=optimizer\n",
    "        self.dtype=tf.float32\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self):\n",
    "        Xnode=tf.constant([4.636567,0.995896],shape=(1,2),dtype=tf.float32)\n",
    "        u_pred=self.u_model(Xnode)\n",
    "        u_pred0=u_pred[:,0]\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x1)\n",
    "            tape.watch(x2)\n",
    "            X_f = tf.stack([x1,x2], axis=1)\n",
    "            u = self.u_model(X_f)\n",
    "            u0=u[:,0]\n",
    "            u1=u[:,1]\n",
    "            u2=u[:,2]\n",
    "    \n",
    "        u_x1=tape.gradient(u0, x1)\n",
    "        #u_x1=tf.reduce_mean(u_x1,axis=1)\n",
    "        u_x1=u_x1+2*x1-2*4.636567\n",
    "        u_x2=tape.gradient(u0, x2)\n",
    "        #u_x2=tf.reduce_mean(u_x2,axis=1)\n",
    "        u_x2=u_x2+2*x2-2*0.995896\n",
    "        \n",
    "        rho=1.0\n",
    "        K=10.0\n",
    "        beta=3.0\n",
    "        x0=1.0\n",
    "        alpha=1.0\n",
    "        lambda0=0.12\n",
    "        R=1.55\n",
    "        sigma1=0.1\n",
    "        sigma2=1.0\n",
    "        b1=rho*x1*(x2-x1/K)-beta*x1/(x1+x0)\n",
    "        b2=R-alpha*x2-lambda0*x1*x2\n",
    "        delta=0.001\n",
    "        \n",
    "        return tf.reduce_mean(tf.square(b1+0.5*sigma1**2*x1**4*u_x1-u1),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(b2+0.5*sigma2**2*u_x2-u2),axis=0) * 1 + \\\n",
    "            tf.reduce_mean((u_x1*u1+u_x2*u2)**2/((u_x1**2+u_x2**2)*(u1**2+u2**2)+delta),axis=0) * 1 + \\\n",
    "            tf.reduce_mean(tf.square(u_pred0),axis=0) * 0.1\n",
    "\n",
    "    def __grad(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss()\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, tf_epochs=5000):\n",
    "    # Creating the tensors\n",
    "    #X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    #u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "    #print(self.__wrap_training_variables())\n",
    "\n",
    "        LOSS=np.zeros([1,tf_epochs])\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad()\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            LOSS[0,epoch]=loss_value.numpy()\n",
    "            if np.mod(epoch,100)==0:\n",
    "                print('epoch, loss_value:', epoch, loss_value)\n",
    "\n",
    "        scio.savemat('LOSS.mat',{'LOSS':LOSS})\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        # f_star = self.H_model()\n",
    "        return u_star#, f_star\n",
    "\n",
    "\n",
    "model=mymodel(layers, tf_optimizer, x1, x2)\n",
    "\n",
    "# checkpoint_save_path=\"./checkpoint/mnist.ckpt\"\n",
    "# if os.path.exists(checkpoint_save_path + '.index'):\n",
    "#   print('-------------------load the model---------------------')\n",
    "#   model.load_weights(checkpoint_save_path)\n",
    "\n",
    "# cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "#                         save_weights_only=True,\n",
    "#                         save_best_only=True)\n",
    "\n",
    "history=model.fit(tf_epochs=1000000)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "x11=tf.constant([4,5.5,7,1.6667,4.636567,7,1.5,4,7],dtype=tf.float32)\n",
    "y11=tf.constant([0,0,0,1.2917,0.995896,1,2,2,2],dtype=tf.float32)\n",
    "X_star = tf.stack([x11,y11], axis=1)\n",
    "print('X_star:\\n ', X_star)\n",
    "u_pred= model.predict(X_star)\n",
    "print('u_pred:\\n ', u_pred)\n",
    "\n",
    "dataFile = \"xtest\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "xtest = data['xtest']\n",
    "dataFile = \"ytest\"  \n",
    "data = scio.loadmat(dataFile)\n",
    "ytest = data['ytest']\n",
    "xtest =tf.reduce_sum(xtest, axis=0)\n",
    "ytest =tf.reduce_sum(ytest, axis=0)\n",
    "X_star = tf.stack([xtest,ytest], axis=1)\n",
    "Stest= model.predict(X_star)\n",
    "scio.savemat('Stest.mat',{'Stest':Stest.numpy()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dedbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing MPEP and prefactor\n",
    "rho=1.0\n",
    "K=10.0\n",
    "beta=3.0\n",
    "x0=1.0\n",
    "alpha=1.0\n",
    "lambda0=0.12\n",
    "R=1.55\n",
    "sigma1=0.1\n",
    "sigma2=1.0\n",
    "detH_bar=0.1546\n",
    "detH_star=0.2380\n",
    "lambda_star=0.3721\n",
    "pi=3.141592653\n",
    "\n",
    "h=0.001\n",
    "nT=10000\n",
    "x1=np.zeros(nT,)\n",
    "x2=np.zeros(nT,)\n",
    "divL=np.zeros(nT,)\n",
    "#normbx=np.zeros(nT,)\n",
    "#x1[0]=1.6667+0.1\n",
    "#x2[0]=1.2917\n",
    "x1[0]=1.6667+0.1*0.8\n",
    "x2[0]=1.2917-0.1*0.6\n",
    "\n",
    "for epoch in range(nT-1):\n",
    "    x01=tf.constant(x1[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(x2[epoch],shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "        u1=u[:,1]\n",
    "        u2=u[:,2]\n",
    "    # Getting the derivative\n",
    "    u_x = tape.gradient(u0, x01)\n",
    "    u_y = tape.gradient(u0, x02)\n",
    "    u_x = u_x+2*x01-2*4.636567\n",
    "    u_y = u_y+2*x02-2*0.995896\n",
    "    lx_x = tape.gradient(u1, x01)\n",
    "    ly_y = tape.gradient(u2, x02)\n",
    "    Ap = 4*sigma1**2*x01**3*u_x\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    divL[epoch]=lx_x.numpy()+ly_y.numpy()+Ap.numpy()\n",
    "    \n",
    "    if ((x1[epoch]-4.636567)**2+(x2[epoch]-0.995896)**2)<0.0025:\n",
    "        break\n",
    "    \n",
    "    p1=u_x.numpy()\n",
    "    p2=u_y.numpy()\n",
    "    bx1=rho*x1[epoch]*(x2[epoch]-x1[epoch]/K)-beta*x1[epoch]/(x1[epoch]+x0)\n",
    "    bx2=R-alpha*x2[epoch]-lambda0*x1[epoch]*x2[epoch]\n",
    "    normb=(bx1**2+bx2**2)**0.5\n",
    "    # normbx[epoch]=normb\n",
    "    x1[epoch+1]=x1[epoch] -h * (bx1 + sigma1**2 *x1[epoch]**4 * p1)/normb\n",
    "    x2[epoch+1]=x2[epoch] -h * (bx2 + sigma2**2 * p2)/normb\n",
    "\n",
    "print('epoch:',epoch)\n",
    "MPEP=tf.stack([x1[0:epoch+1],x2[0:epoch+1]], axis=1)\n",
    "scio.savemat('MPEP.mat',{'MPEP':MPEP.numpy()})\n",
    "print(MPEP)\n",
    "integ=np.sum(divL[0:epoch+1])*h\n",
    "prefactor=pi/lambda_star*(detH_star/detH_bar)**0.5*np.exp(integ)\n",
    "print('integ:',integ)\n",
    "print('prefactor:',prefactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing MFPT\n",
    "Dinv=40\n",
    "xnode1=1.6667\n",
    "xnode2=1.2917\n",
    "xnode1=tf.constant(xnode1,shape=(1,),dtype=tf.float32)\n",
    "xnode2=tf.constant(xnode2,shape=(1,),dtype=tf.float32)\n",
    "X_f = tf.stack([xnode1,xnode2], axis=1)\n",
    "u = model.predict(X_f)\n",
    "u0=u[:,0]\n",
    "U0=u0+(xnode1-4.636567)**2+(xnode2-0.995896)**2\n",
    "MFPT=prefactor*np.exp(U0*Dinv)\n",
    "print('U0:',U0)\n",
    "print('MFPT:',MFPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding the location of minimal quasi-potential on the boundary x1=3\n",
    "rho=1.0\n",
    "K=10.0\n",
    "beta=3.0\n",
    "x0=1.0\n",
    "alpha=1.0\n",
    "lambda0=0.12\n",
    "R=1.55\n",
    "sigma1=0.1\n",
    "sigma2=1.0\n",
    "\n",
    "xm1=3.0\n",
    "xm2=1.0\n",
    "eta=0.01\n",
    "\n",
    "for epoch in range(1000):\n",
    "    x01=tf.constant(xm1,shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(xm2,shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "    # Getting the derivative\n",
    "    u_x2 = tape.gradient(u0, x02)\n",
    "    u_x2 = u_x2+2*(x02-0.995896)\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    \n",
    "    p2=u_x2.numpy()\n",
    "    xm2=xm2-eta*p2\n",
    "    print(epoch,xm1,xm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa10606",
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing MPEP and prefactor non-characteristic boundary\n",
    "x01=tf.constant(xm1,shape=(1,),dtype=tf.float32)\n",
    "x02=tf.constant(xm2,shape=(1,),dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x01)\n",
    "    tape.watch(x02)\n",
    "    X_f = tf.stack([x01,x02], axis=1)\n",
    "    # Getting the prediction\n",
    "    u = model.predict(X_f)\n",
    "    u0=u[:,0]\n",
    "    u1=u[:,1]\n",
    "    u2=u[:,2]\n",
    "    u_x2 = tape.gradient(u0, x02)\n",
    "# Getting the derivative\n",
    "u_x1 = tape.gradient(u0, x01)\n",
    "u_x1 = u_x1+2*(x01-4.636567)\n",
    "u_x2x2 = tape.gradient(u_x2, x02)\n",
    "u_x2x2 = u_x2x2+2\n",
    "\n",
    "detH_bar=0.1546\n",
    "deth2_star=sigma2**2*u_x2x2.numpy()\n",
    "miu2_star=-(0.5*sigma1**2*xm1**4*u_x1.numpy()+u1.numpy())\n",
    "pi=3.141592653\n",
    "print('deth2_star:',deth2_star)\n",
    "print('miu2_star:',miu2_star)\n",
    "\n",
    "h=0.001\n",
    "nT=10000\n",
    "x1=np.zeros(nT,)\n",
    "x2=np.zeros(nT,)\n",
    "divL=np.zeros(nT,)\n",
    "#normbx=np.zeros(nT,)\n",
    "x1[0]=xm1\n",
    "x2[0]=xm2\n",
    "\n",
    "for epoch in range(nT-1):\n",
    "    x01=tf.constant(x1[epoch],shape=(1,),dtype=tf.float32)\n",
    "    x02=tf.constant(x2[epoch],shape=(1,),dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x01)\n",
    "        tape.watch(x02)\n",
    "        # Packing together the inputs\n",
    "        X_f = tf.stack([x01,x02], axis=1)\n",
    "        # Getting the prediction\n",
    "        u = model.predict(X_f)\n",
    "        u0=u[:,0]\n",
    "        u1=u[:,1]\n",
    "        u2=u[:,2]\n",
    "    # Getting the derivative\n",
    "    u_x = tape.gradient(u0, x01)\n",
    "    u_y = tape.gradient(u0, x02)\n",
    "    u_x = u_x+2*x01-2*4.636567\n",
    "    u_y = u_y+2*x02-2*0.995896\n",
    "    lx_x = tape.gradient(u1, x01)\n",
    "    ly_y = tape.gradient(u2, x02)\n",
    "    Ap = 4*sigma1**2*x01**3*u_x\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    divL[epoch]=lx_x.numpy()+ly_y.numpy()+Ap.numpy()\n",
    "    \n",
    "    if ((x1[epoch]-4.636567)**2+(x2[epoch]-0.995896)**2)<0.01:\n",
    "        break\n",
    "    \n",
    "    p1=u_x.numpy()\n",
    "    p2=u_y.numpy()\n",
    "    bx1=rho*x1[epoch]*(x2[epoch]-x1[epoch]/K)-beta*x1[epoch]/(x1[epoch]+x0)\n",
    "    bx2=R-alpha*x2[epoch]-lambda0*x1[epoch]*x2[epoch]\n",
    "    normb=(bx1**2+bx2**2)**0.5\n",
    "    # normbx[epoch]=normb\n",
    "    x1[epoch+1]=x1[epoch] -h * (bx1 + sigma1**2 *x1[epoch]**4 * p1)/normb\n",
    "    x2[epoch+1]=x2[epoch] -h * (bx2 + sigma2**2 * p2)/normb\n",
    "\n",
    "print('epoch:',epoch)\n",
    "MPEP0=tf.stack([x1[0:epoch+1],x2[0:epoch+1]], axis=1)\n",
    "scio.savemat('MPEP0.mat',{'MPEP0':MPEP0.numpy()})\n",
    "print(MPEP0)\n",
    "integ=np.sum(divL[0:epoch+1])*h\n",
    "prefactor0=1/miu2_star*(2*pi*deth2_star/detH_bar)**0.5*np.exp(integ)\n",
    "print('integ:',integ)\n",
    "print('prefactor0:',prefactor0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a040ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing MFPT\n",
    "Dinv=40\n",
    "xnode1=xm1\n",
    "xnode2=xm2\n",
    "xnode1=tf.constant(xnode1,shape=(1,),dtype=tf.float32)\n",
    "xnode2=tf.constant(xnode2,shape=(1,),dtype=tf.float32)\n",
    "X_f = tf.stack([xnode1,xnode2], axis=1)\n",
    "u = model.predict(X_f)\n",
    "u0=u[:,0]\n",
    "U0=u0+(xnode1-4.636567)**2+(xnode2-0.995896)**2\n",
    "MFPT=prefactor0*(1/Dinv)**0.5*np.exp(U0*Dinv)\n",
    "print('U0:',U0)\n",
    "print('MFPT:',MFPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a64318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
